# Cogs118B-Final-Project
This paper is based on an earlier project by Curana and Niculescu-Mizil. They iterate through eight algorithms and datasets to compare the relative abilities of each algorithm. Their paper guides users to choose algorithms and consider what the different classifiers output: probabilities, class labels, etc. These are compared across different binary classification datasets and different metrics. For the individual version of this project, only three classification algorithms, four datasets, and three metric measurements are required. Caruana gives the groundwork of how to format the information coming from the algorithms to render them comparable. Statistical analysis is used to further represent the merits of this approach.

CIntroduction
Caruana was one of the first large scale papers comparing learning algorithms. I evaluate the performance of SVMs, Logistic Regression, and K nearest neighbors. These algorithms are compared on the performance metrics F-score, accuracy, and ROC area. SVM maximizes the margin between the decision boundary line and the positive/negative class lines (Fleischer). In order to account for linearly inseparable data, SVM varies over regularization parameter C. Apart from linear SVM, I also analyze polynomial SVM, degrees 2 and 3, and rbf SVM, which uses lagrangian math to determine optimal support vectors. These decision boundaries aren’t linear. For Logistic regression, the algorithm is outputting a probability of a datapoint belonging to a class. This is helpful to know the confidence in your prediction. Logistic regression utilizes the sigmoid logistic function to have a loss function that is differentiable everywhere (Fleischer). KNN is very different from these two. With fast training time, but very slow testing time, this algorithm compares the distance of a testing point to the distances of all training points. For KNN, I iterate over both Euclidean distance and Manhattan distance. KNN labels a point based on the class labels of the K nearest training neighbors. 
Method
Learning Algorithms
SVMs: I use the following kernels: linear, polynomial degree 2 & 3, rbf with gamma {0.001,0.005,0.01,0.05,0.1,0.5,1,2}. I changed the regularization parameter, C, by factors of ten from 10−7 to 103 for each kernel.
Logistic Regression (LOGREG): I trained with both unregularized and regularized models. For regularization, I used both L1 and L2. The solver also varied from ‘saga’ to ‘lbfgs’; keeping the max iterations at 5000. The regularization parameter varied by factors of 10 from 10^−8 to 10^4.
K-Nearest Neighbor(KNN): I used 5 values of K; 5, 10, 20, 200, 1000; this was different from the Caruana paper and was done to help lower computation time. I began to run out of time towards the end of this project. I use KNN with Euclidean distance and Manhattan distance. The weights were also varied: uniform and distance. 
Datasets
To begin, I download the three datasets from the UCI repository; the datasets are titled LETTER, ADULT, and COV_TYPE. To create a binary classification problem, for the LETTER dataset, I divided the twenty six classes into two. For the first problem, the letter ‘O’ was labeled as the positive class and the rest of the letters were negative. In the second problem, letters ‘A’- ‘M’ are labeled as positive and ‘N’- ‘Z’ as negative. Since the first problem is not evenly distributed, I made sure to use stratified kfold. This created two of the binary datasets that I would test on. For the second dataset, COV-TYPE, I read through the data info writeup and labeled the most popular class, group 2, as the positive class. I also renamed the columns to  better understand the data features being used. For the Adult dataset, I used pandas ‘get_dummies’ function to one hot encode categorical variables. I also changed the salary income to get a binary classification problem. Table 1 shows a breakdown of the datasets. 

Experiment
From there, I began by creating three notebooks, one for each type of classifier, so that the notebooks could run in parallel. Each classifier and each dataset had 5 trials with 5 folds of cross validation. For each trial, I selected five thousand samples to be the training and validation set. I then created a pipeline to standardize the data. I compared on three metrics: F1 Micro score, accuracy, and roc auc curve. Table 2, just like Caruana, shows the normalized score for each algorithm on my three selected metrics. For each problem and metric we find the best parameter settings for each algorithm using the 1k validation sets set aside by cross- validation, then report that model’s normalized score on the final test set. 
In the table, higher algorithms indicate better performance. The last column, MEAN, is the mean normalized score over the three metrics, four problems, and five trials. In the table, the algorithm with the best performance on each metric is boldfaced. Interestingly, SVM and KNN had the same average score for accuracy. Overall, SVM had the best mean score.  The data points with an * were found to be statistically significant at the p = .05 level. I did an unpaired t test because I did not assign the same seed for each trial across algorithms. The ADULT problem set t-test was corrupted by the excessive number of ones received. I compared each mean to the highest mean for ever algorithms, problem set and metric with p values for each output; the p values are in appendix b. With very low standard deviations, not many of them were found to be statistically significant.
Table 3 shows the normalized score for each algorithm on each of the 3 test metrics. Each entry is an average over the four datasets and five trials.
Discussion
For both SVM and Logistic Regression, I received a score of 1 for the adult dataset metrics. I ran these algorithms after KNN, so I was not aware of an issue with the ADULT dataset cleaning I did. These performance metrics are too high for chance. Overall the algorithms ran better on LETTER and ADULT datasets. The specific results depend on the metric and problem, but overall SVM performed the best. The bootstrap analysis complements the t-tests in Tables 2 and 3. The results suggest that if we had selected other problems/metrics, there is a large chance that SVM would still have ranked 1st. Overall it is important to understand the No Free Lunch theorem; there is no one general purpose algorithm. It is important to understand the problem and what you want to get out of it. 
References
Caruana, R., & Niculescu-Mizil, A. (2006, June). An empirical comparison of supervised learning algorithms. In 
Proceedings of the 23rd international conference on Machine learning (pp. 161-168).
J. Fleischer. Support Vector Machines. COGS 118A (Winter 2021).
J. Fleischer. Logistic Regression. COGS 118A (Winter 2021).
